---
layout:  pub
title:   'Reconciling Shannon and Scott with a Lattice of Computable Information'
authors:
  - Sebastian Hunt
  - David Sands
  - Sandro Stucki
howpublished: "To appear in <em>Proceedings of the ACM on Programming Languages</em>, 7(POPL)"
copynote:
  'pp. 68:1&ndash;68:30, ACM, 2023'
links:
  '<span style="white-space:nowrap">DOI (OA)</span>':
    https://doi.org/10.1145/3571740
  Preprint: https://arxiv.org/abs/2211.10099
  BibTeX: /bib/HuntSS23popl.bib
---

## Abstract

This paper proposes a reconciliation of two different theories of information. The first, originally proposed in a lesser-known work by Claude Shannon (some five years after the publication of his celebrated quantitative theory of communication), describes how the information content of channels can be described _qualitatively_, but still abstractly, in terms of _information elements_, where information elements can be viewed as equivalence relations over the data source domain. Shannon showed that these elements have a partial ordering, expressing when one information element is more informative than another, and that these partially ordered information elements form a complete lattice.  In the context of security and information flow this structure has been independently rediscovered several times, and used as a foundation for understanding and reasoning about information flow.

The second theory of information is Dana Scott's domain theory, a mathematical framework for giving meaning to programs as continuous functions over a particular topology. Scott's partial ordering also represents when one element is more informative than another, but in the sense of computational progress &ndash; i.e. when one element is a more defined or evolved version of another.

To give a satisfactory account of information flow in computer programs it is necessary to consider both theories together, in order to understand not only what information is conveyed by a program (viewed as a channel, à la Shannon) but also how the precision with which that information can be observed is determined by the definedness of its encoding (à la Scott).  To this end we show how these theories can be fruitfully combined, by defining _the Lattice of Computable Information_ (LoCI), a lattice of preorders rather than equivalence relations. LoCI retains the rich lattice structure of Shannon's theory, filters out elements that do not make computational sense, and refines the remaining information elements to reflect how Scott's ordering captures possible varieties in the way that information is presented.

We show how the new theory facilitates the first general definition of termination-insensitive information flow properties, a weakened form of information flow property commonly targeted by static program analyses.
